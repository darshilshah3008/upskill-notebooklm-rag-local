# ğŸ“˜ **upskill-notebooklm-rag-local**

A fully local, privacy-friendly **NotebookLM-style RAG** system using **Ollama**, **FAISS**, and **Python**.  
No cloud. No API keys. 100% offline.

---

## ğŸš€ **Features**
- 100% local (offline)  
- PDF â†’ Text â†’ Chunks â†’ Embeddings â†’ Answers  
- OCR for scanned PDFs  
- Semantic search via **FAISS**  
- Local LLM responses using **Ollama**  
- Simple, modular structure  

---

## ğŸ“ **Project Structure**
```
upskill-notebooklm-rag-local/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ pdfs/      # Add your PDFs here
â”‚   â””â”€â”€ index/     # Auto-generated FAISS index
â”œâ”€â”€ ingest.py      # Text extraction + OCR + chunking
â”œâ”€â”€ embedding.py   # Embeddings + FAISS builder
â”œâ”€â”€ search.py      # Semantic search
â”œâ”€â”€ rag.py         # NotebookLM-style chat
â””â”€â”€ utils.py       # Ollama utilities
```

---

## ğŸ§© **Requirements**
- Python **3.10+**  
- Ollama â†’ https://ollama.com/download  
- **16GB RAM** minimum (7B models load but slow on CPU)  
- **Recommended for speed:**  
  - NVIDIA GPU / Apple Silicon / Jetson Orin  
- **CPU-friendly models:**  
  ```
  qwen2.5:3b-instruct
  phi3:mini
  ```

---

## âš™ï¸ **Installation**
```
git clone your-repo-url
cd upskill-notebooklm-rag-local
python -m venv .venv
source .venv/bin/activate   # or .venv/Scripts/activate on Windows
pip install -r requirements.txt
```

---

## ğŸ§  **Models to Install**
### LLM (answers)
```
ollama pull mistral
```
Or faster:
```
ollama pull qwen2.5:3b-instruct
```

### Embedding Model
```
ollama pull nomic-embed-text
```

---

## ğŸ“¥ **Add PDFs**
Put files inside:
```
data/pdfs/
```

---

## ğŸ—ï¸ **Build FAISS Index**
```
python rag.py --ingest
```

---

## ğŸ’¬ **Start Chatting**
```
python rag.py
```

Example:
```
Q> Summarize page 3.
```

---

## ğŸ **Done**
Your offline NotebookLM-style RAG system is readyâ€”private, fast, and ideal for technical PDFs, engineering docs, and research papers.
