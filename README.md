# ğŸ“˜ **upskill-notebooklm-rag-local**

A fully local, privacy-friendly **NotebookLM-style RAG system** using **Ollama**, **FAISS**, and **Python**.  
No cloud. No API keys. 100% offline.

---

## ğŸš€ **Features**
- 100% local (no internet required)  
- PDF â†’ Text â†’ Chunks â†’ Embeddings â†’ Answers  
- OCR fallback for scanned PDFs  
- Semantic search powered by **FAISS**  
- Local LLM answering using **Ollama**  
- Beginner-friendly modular code  

---

## ğŸ“ **Project Structure**
```
upskill-notebooklm-rag-local/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ pdfs/          # Put your PDFs here
â”‚   â””â”€â”€ index/         # Auto-generated embeddings + FAISS index
â”‚
â”œâ”€â”€ ingest.py          # Extract text, OCR, chunking
â”œâ”€â”€ embedding.py       # Embedding + FAISS index builder
â”œâ”€â”€ search.py          # Retriever (semantic search)
â”œâ”€â”€ rag.py             # NotebookLM-style chat interface
â”œâ”€â”€ utils.py           # Ollama config + chat/embedding utilities
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ§© **Requirements**

### **Software**
- Python **3.10+**  
- Ollama â†’ https://ollama.com/download  

### **Hardware Notes**
- **16GB RAM is enough to load a 7B model (Q4)**  
  but **CPU-only inference will be slow**.  
- For fast performance:
  - NVIDIA GPU (RTX 20/30/40 series)  
  - Apple Silicon (M1/M2/M3)  
  - Jetson Orin Nano / Orin NX  
- For CPU-only laptops, use smaller models:
  ```
  qwen2.5:3b-instruct
  phi3:mini
  tinyllama:1.1b
  ```
- **32GB+ RAM recommended** for smooth ingestion & large PDFs.

---

## âš™ï¸ **Installation**
```
git clone your-repo-url
cd upskill-notebooklm-rag-local
python -m venv .venv
```

### Windows
```
. .venv/Scripts/activate
```

### macOS / Linux
```
source .venv/bin/activate
```

### Install dependencies
```
pip install -r requirements.txt
```

---

## ğŸ§  **Install Ollama Models**

### Answering Model (LLM)
```
ollama pull mistral
```

Why?  
âœ” Good RAG quality  
âœ” Offline  
âœ” Strong reasoning  

â— *Note:* Slow on CPU laptops.

### Faster models:
```
ollama pull qwen2.5:3b-instruct
ollama pull phi3:mini
```

---

### Embedding Model
```
ollama pull nomic-embed-text
```

Why?  
âœ” Fast  
âœ” Lightweight  
âœ” Excellent for semantic search  

Embedding â†’ creates vectors  
LLM â†’ generates answers  

Both are required.

---

## ğŸ“¥ **Add Your PDFs**
Place your documents into:
```
data/pdfs/
```

---

## ğŸ—ï¸ **Build FAISS Index**
*(One-time per PDF set)*

```
python rag.py --ingest
```

This will:
- Extract text  
- Perform OCR  
- Chunk the text  
- Generate embeddings  
- Build FAISS index  

Stored in:
```
data/index/
```

---

## ğŸ’¬ **Start Asking Questions**
```
python rag.py
```

Example usage:
```
Q> What is this PDF about?
Q> Summarize page 3.
Q> What are the steps in this section?
Q> exit
```

---

## ğŸ”§ **Troubleshooting**

### TimeoutError
Use a smaller model:
```
qwen2.5:3b-instruct
```
Or set:
```
timeout=None
```
in `utils.py`.

### Slow responses?
Use:
```
qwen2.5:3b-instruct
phi3:mini
```

### Need higher quality?
Use:
```
qwen2.5:7b-instruct
llama3.1:8b-instruct
```

---

## ğŸ **Conclusion**
Your fully local **NotebookLM-style RAG system** is ready.  
Add PDFs â†’ Ingest â†’ Chat.

Perfect for:
- Engineering manuals  
- Embedded & automotive docs  
- Research papers  
- Books & tutorials  
- Private knowledge bases  
