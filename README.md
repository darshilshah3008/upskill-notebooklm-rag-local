ğŸ“˜ upskill-notebooklm-rag-local

A fully local, privacy-friendly NotebookLM-style RAG system using Ollama, FAISS, and Python.

ğŸš€ Features

100% local (no internet required)

PDF â†’ Text â†’ Chunks â†’ Embeddings â†’ Answers

OCR fallback for scanned PDFs

Semantic search using FAISS

Local LLM answering using Ollama

Beginner-friendly workflow

ğŸ“ Project Structure
upskill-notebooklm-rag-local/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ pdfs/          # Put your PDFs here
â”‚   â””â”€â”€ index/         # Auto-generated embeddings + FAISS index
â”‚
â”œâ”€â”€ ingest.py          # Extract text, OCR, chunking
â”œâ”€â”€ embedding.py       # Embedding + FAISS index builder
â”œâ”€â”€ search.py          # Retriever (semantic search)
â”œâ”€â”€ rag.py             # Main NotebookLM-style chat
â”œâ”€â”€ utils.py           # Ollama config + chat/embedding utilities
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

ğŸ§© Requirements
Software

Python 3.10+

Ollama installed â†’ https://ollama.com/download

Hardware Notes

16GB RAM is enough to load a 7B model (Q4 quantized)
but CPU-only inference will be slow (0.5â€“2 tokens/sec).

For fast performance, use:

NVIDIA GPU (RTX 20/30/40 series)

Apple Silicon (M1/M2/M3)

Jetson Orin Nano / Orin NX

For CPU-only laptops, use smaller models:

qwen2.5:3b-instruct
phi3:mini
tinyllama:1.1b


32GB+ RAM recommended for handling large PDFs smoothly.

âš™ï¸ Installation
git clone your-repo-url
cd upskill-notebooklm-rag-local
python -m venv .venv

# Windows
. .venv/Scripts/activate  

# macOS / Linux
source .venv/bin/activate

pip install -r requirements.txt

ğŸ§  Install Ollama Models
Answering Model (LLM)
ollama pull mistral


Why?
âœ” Good quality
âœ” Strong for RAG
âœ” Works offline

â— Note: Mistral 7B is slow on CPU-only laptops.

Faster alternatives:

ollama pull qwen2.5:3b-instruct
ollama pull phi3:mini

Embedding Model
ollama pull nomic-embed-text


Why?
âœ” Fast
âœ” Lightweight
âœ” Excellent for semantic PDF search

Embedding model â†’ creates text vectors
LLM â†’ generates final answers

Both are required.

ğŸ“¥ Add Your PDFs

Place your documents into:

data/pdfs/


Supports text PDFs, scanned PDFs (OCR), and large manuals.

ğŸ—ï¸ Build FAISS Index (One-time per new PDF set)
python rag.py --ingest


This process:

Extracts text

Runs OCR if needed

Splits text into chunks

Generates embeddings

Builds FAISS vector index

Index files are stored in:

data/index/

ğŸ’¬ Start Asking Questions
python rag.py


Example queries:

Q> What is this PDF about?
Q> Summarize page 3.
Q> What are the steps of this project?
Q> exit

ğŸ”§ Troubleshooting
â— TimeoutError

Use a smaller LLM:

ollama pull qwen2.5:3b-instruct


Or set:

timeout=None


inside utils.py.

â— Slow Answers

Use:

qwen2.5:3b-instruct
phi3:mini

â— Need Better Quality?

Use:

qwen2.5:7b-instruct
llama3.1:8b-instruct

ğŸ Conclusion

Your fully local NotebookLM-style RAG system is ready.
Just add PDFs â†’ ingest â†’ chat.

Ideal for:

Engineering manuals

Embedded & automotive documents

Research papers

Books & tutorials

Private knowledge bases
