
# ğŸ“˜ upskill-notebooklm-rag-local

A fully local, privacy-friendly **NotebookLM-style RAG system** using **Ollama**, FAISS, and Python.

## ğŸš€ Features
- 100% local (no internet required)
- PDF â†’ Text â†’ Chunks â†’ Embeddings â†’ Answers
- OCR fallback for scanned PDFs
- Semantic search using FAISS
- Local LLM answering using Ollama
- Beginner-friendly workflow

## ğŸ“ Project Structure
```
upskill-notebooklm-rag-local/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ pdfs/          # Put your PDFs here
â”‚   â””â”€â”€ index/         # Auto-generated embeddings + FAISS index
â”‚
â”œâ”€â”€ ingest.py          # Extract text, OCR, chunking
â”œâ”€â”€ embedding.py       # Embedding + FAISS index builder
â”œâ”€â”€ search.py          # Retriever (semantic search)
â”œâ”€â”€ rag.py             # Main NotebookLM-style chat
â”œâ”€â”€ utils.py           # Ollama config + chat/embedding utilities
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ§© Requirements
- Python 3.10+
- Ollama installed â†’ https://ollama.com/download  
- 16GB RAM minimum  
- 32â€“48GB RAM recommended (for faster models)

## âš™ï¸ Installation
```
git clone your-repo-url
cd upskill-notebooklm-rag-local
python -m venv .venv
.venv\Scripts\activate
pip install -r requirements.txt
```

## ğŸ§  Install Ollama Models

### Answering Model (LLM)
```
ollama pull mistral
```
Why?  
âœ” Lightweight  
âœ” Good quality  
âœ” Fast on CPU  
âœ” Perfect for local RAG

### Embedding Model
```
ollama pull nomic-embed-text
```
Why?  
âœ” Fast  
âœ” Small  
âœ” Works perfectly for PDF search

Embedding model â†’ Converts text into vectors  
LLM â†’ Generates human-like answers  
Both are required.

## ğŸ“¥ Add Your PDFs
Place your files into:
```
data/pdfs/
```

## ğŸ—ï¸ Build FAISS Index (Oneâ€‘time per new PDF set)
```
python rag.py --ingest
```

This:
- Extracts text  
- Runs OCR if needed  
- Creates chunks  
- Embeds via Ollama  
- Builds FAISS vector index  

## ğŸ’¬ Start Asking Questions
```
python rag.py
```

Example queries:
```
Q> What is this PDF about?
Q> Summarize page 3.
Q> What are the steps of this project?
Q> exit
```

## ğŸ”§ Troubleshooting

### â— TimeoutError
Use smaller LLM:
```
ollama pull qwen2.5:3b-instruct
```
Or set timeout=None in utils.py.

### â— Slow Answers
Use:
```
qwen2.5:3b-instruct  (fastest)
```

### â— Better Quality
Use:
```
qwen2.5:7b-instruct
llama3.1:8b-instruct
```

## ğŸ Conclusion
Your local NotebookLM-style RAG system is ready.  
Just add PDFs â†’ Ingest â†’ Chat.
